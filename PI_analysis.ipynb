{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data\n",
    "Data has been acquired fron Stanford HIV database. \n",
    "It is a genotype-phenotype correlation dataset that contains isolates on which in vitro susceptibility tests were performed using the PhenoSense assay. Protease inhibitor resistance dataset is the one being studied.\n",
    "The link to reaquire data is below. \n",
    "\n",
    "curl -o PI_dataset.txt https://hivdb.stanford.edu/download/GenoPhenoDatasets/PI_DataSet.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.13/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (2.2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.13/site-packages (4.50.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./.venv/lib/python3.13/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.13/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch) (77.0.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (2.2.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installing the required packages\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Formatted Sequence:\n",
      "P Q I T L W Q R P L V T I K I G G Q L K E A L L D T G A D N T V L E E M N L P G R W K P K M I G G I G G F I K V G Q Y D Q I L I E I C G H K A I G T V L V G P T P V N I I G R D L L T Q I G C T L N F\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the consensus sequence for the protease inhibitor\n",
    "protease_consensus = \"PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMNLPGRWKPKMIGGIGGFIKVRQYDQILIEICGHKAIGTVLVGPTPVNIIGRNLLTQIGCTLNF\"\n",
    "\n",
    "# List of drug columns in the dataset\n",
    "drug_cols = [\"FPV\", \"ATV\", \"IDV\", \"LPV\", \"NFV\", \"SQV\", \"TPV\", \"DRV\"]\n",
    "\n",
    "# Define drug-specific resistance thresholds (fold change)\n",
    "# (References: Pironti et al., JAIDS 2017; Shen et al., 2016)\n",
    "drug_thresholds = {\n",
    "    \"FPV\": 4.0,\n",
    "    \"ATV\": 3.0,\n",
    "    \"IDV\": 3.0,\n",
    "    \"LPV\": 9.0,\n",
    "    \"NFV\": 3.0,\n",
    "    \"SQV\": 3.0,\n",
    "    \"TPV\": 2.0,\n",
    "    \"DRV\": 10.0\n",
    "}\n",
    "\n",
    "#Load the dataset from the txt file we downloaded\n",
    "df = pd.read_csv(\"PI_dataset.txt\", sep=\"\\t\")\n",
    "\n",
    "# Verify that the sequence columns exist.\n",
    "# We'll filter columns whose names start with 'P' and are followed by digits.\n",
    "# This assumes that the sequence columns are named 'P1', 'P2', ..., 'P99'.\n",
    "p_columns = [col for col in df.columns if col.startswith(\"P\") and col[1:].isdigit()]\n",
    "\n",
    "# Sort the columns in numerical order (P1, P2, ..., P99)\n",
    "p_columns = sorted(p_columns, key=lambda x: int(x[1:]))\n",
    "\n",
    "# Check that we have exactly 99 positions (P1 to P99)\n",
    "# This should be the case as protease is a homodimer with each subunit having 99 amino acids \n",
    "if len(p_columns) != 99:\n",
    "    print(f\"Warning: Expected 99 sequence positions but found {len(p_columns)}\")\n",
    "\n",
    "def concatenate_sequence(row, columns, consensus_seq):\n",
    "    \"\"\"\n",
    "    Concatenate amino acid columns into a single sequence string.\n",
    "    Handling missing/ambiguous symbols (from the dataset description):\n",
    "      - If a cell is NaN, replace with 'X' (unknown).\n",
    "      - If the value is '.', replace with 'X' (unknown, no sequence data).\n",
    "      - If the value is '-', replace with the corresponding consensus residue.\n",
    "    \"\"\"\n",
    "    seq_list = []\n",
    "    # Enumerate over the sorted columns so we know the position (0-indexed)\n",
    "    for idx, col in enumerate(columns):\n",
    "        aa = row[col]\n",
    "        if pd.isna(aa):\n",
    "            # If the value is NaN, mark as unknown\n",
    "            aa = 'X'\n",
    "        else:\n",
    "            aa = str(aa).strip()\n",
    "            # Replace '.' with unknown, and '-' with the consensus residue\n",
    "            if aa == '.':\n",
    "                aa = 'X'\n",
    "            elif aa == '-':\n",
    "                aa = consensus_seq[idx]\n",
    "        seq_list.append(aa)\n",
    "    \n",
    "    # Join the amino acids into a continuous string (without spaces)\n",
    "    raw_seq = ''.join(seq_list)\n",
    "    \n",
    "    # Insert spaces between each amino acid for ProteinBERT tokenization\n",
    "    formatted_seq = \" \".join(list(raw_seq))\n",
    "    \n",
    "    return formatted_seq\n",
    "\n",
    "# Apply the function to each row to create a new column with the formatted sequence\n",
    "df[\"FormattedSequence\"] = df.apply(lambda row: concatenate_sequence(row, p_columns, protease_consensus), axis=1)\n",
    "\n",
    "# Display a sample formatted sequence\n",
    "print(\"Sample Formatted Sequence:\")\n",
    "print(df.loc[0, \"FormattedSequence\"])\n",
    "\n",
    "# Create binary labels for each drug: 1 if fold change >= threshold (resistant), else 0 (susceptible)\n",
    "for drug in drug_cols:\n",
    "    df[f\"{drug}_label\"] = df[drug].apply(lambda x: 1 if x >= drug_thresholds[drug] else 0)\n",
    "\n",
    "# Create a combined label column as an 8-dimensional vector (order: FPV, ATV, IDV, LPV, NFV, SQV, TPV, DRV)\n",
    "label_cols = [f\"{drug}_label\" for drug in drug_cols]\n",
    "labels = df[label_cols].values  # shape: (num_samples, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "sample_encoding = tokenizer(\n",
    "    df.loc[0, \"FormattedSequence\"],\n",
    "    max_length=128,  # Adjust this based on your sequence length plus special tokens\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(sample_encoding.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 / 5\n",
      "Epoch 1 Loss: 0.5165\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 142\u001b[39m\n\u001b[32m    140\u001b[39m loss = loss_fn(logits, labels_batch)\n\u001b[32m    141\u001b[39m total_loss += loss.item()\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m optimizer.step()\n\u001b[32m    144\u001b[39m scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local Gits/HIV_classifier/.venv/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local Gits/HIV_classifier/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local Gits/HIV_classifier/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "class HIVMultiLabelDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, tokenizer, max_length=128):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        label = self.labels[idx]  # label is an array of length 8\n",
    "        encoding = self.tokenizer(\n",
    "            seq,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Squeeze the batch dimension for each tensor.\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        # Convert label to tensor (float for BCEWithLogitsLoss)\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# Prepare the list of sequences from \"FormattedSequence\"\n",
    "sequences = df[\"FormattedSequence\"].tolist()\n",
    "\n",
    "# ================================\n",
    "# 4. DEFINE THE MULTI-LABEL CLASSIFIER MODEL\n",
    "# ================================\n",
    "# We define a custom classifier that uses the ProtBERT model as the base.\n",
    "# The classifier will have 8 outputs (one for each drug), and we use BCEWithLogitsLoss.\n",
    "class ProteinBERTMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=8, dropout_prob=0.1):\n",
    "        super(ProteinBERTMultiLabelClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # The hidden size of ProtBERT is typically 1024 (check model info)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the pooled output (corresponds to [CLS] token representation)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# ================================\n",
    "# 5. TRAINING WITH CROSS-VALIDATION\n",
    "# ================================\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 3\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "max_length = 128  # Adjust as needed (should be >= number of tokens in formatted sequence)\n",
    "\n",
    "# Set device \n",
    "# Testing this on my mac first, then will run on MBI server \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"Using 'mps' (multi-process service) device\")\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Prepare multi-label labels array\n",
    "multi_labels = labels  # shape (N, 8)\n",
    "\n",
    "# Convert sequences and labels to lists for dataset creation\n",
    "dataset_sequences = sequences\n",
    "dataset_labels = multi_labels\n",
    "\n",
    "# Set up 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_acc_list = []\n",
    "fold_f1_list = []\n",
    "\n",
    "# Function to compute multi-label accuracy (exact match) and F1 score (macro)\n",
    "def compute_metrics(true_labels, pred_probs, threshold=0.5):\n",
    "    pred_labels = (pred_probs >= threshold).astype(int)\n",
    "    # Exact match accuracy (all labels correct)\n",
    "    exact_match_acc = np.mean(np.all(pred_labels == true_labels, axis=1))\n",
    "    # Compute macro F1 (average F1 over labels)\n",
    "    f1s = []\n",
    "    for i in range(true_labels.shape[1]):\n",
    "        f1s.append(f1_score(true_labels[:, i], pred_labels[:, i], zero_division=0))\n",
    "    macro_f1 = np.mean(f1s)\n",
    "    return exact_match_acc, macro_f1\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset_sequences)):\n",
    "    print(f\"\\nFold {fold+1} / 5\")\n",
    "    \n",
    "    train_seqs = [dataset_sequences[i] for i in train_idx]\n",
    "    train_lbls = dataset_labels[train_idx]\n",
    "    val_seqs = [dataset_sequences[i] for i in val_idx]\n",
    "    val_lbls = dataset_labels[val_idx]\n",
    "    \n",
    "    train_dataset = HIVMultiLabelDataset(train_seqs, train_lbls, tokenizer, max_length=max_length)\n",
    "    val_dataset = HIVMultiLabelDataset(val_seqs, val_lbls, tokenizer, max_length=max_length)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize the model for this fold\n",
    "    model = ProteinBERTMultiLabelClassifier(num_labels=8)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Training loop for the current fold\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_batch = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(logits, labels_batch)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Evaluation on validation set\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_batch = batch['labels'].cpu().numpy()\n",
    "            \n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.sigmoid(logits).cpu().numpy()  # get probabilities\n",
    "            all_preds.append(preds)\n",
    "            all_trues.append(labels_batch)\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_trues = np.vstack(all_trues)\n",
    "    \n",
    "    fold_acc, fold_f1 = compute_metrics(all_trues, all_preds, threshold=0.5)\n",
    "    print(f\"Fold {fold+1} Exact Match Accuracy: {fold_acc:.4f}, Macro F1: {fold_f1:.4f}\")\n",
    "    fold_acc_list.append(fold_acc)\n",
    "    fold_f1_list.append(fold_f1)\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Overall results\n",
    "print(\"\\nCross-validation Results:\")\n",
    "print(f\"Average Exact Match Accuracy: {np.mean(fold_acc_list):.4f} ± {np.std(fold_acc_list):.4f}\")\n",
    "print(f\"Average Macro F1 Score: {np.mean(fold_f1_list):.4f} ± {np.std(fold_f1_list):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
